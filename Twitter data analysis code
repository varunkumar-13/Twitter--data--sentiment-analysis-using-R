
#Loading all the required R libraries

library(twitteR)
library(ROAuth)
library(hms)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(textdata)

#Twitter authorization to extract tweets

consumer_key <-"HJFjGj4jePPbVlmHRwqUUA2hR"
consumer_secret <-"iMwL77EecWmwJcjZ2IJcACtbKeqrU330pNARSrJDTznLblfyA1"
access_token<-"1332340495473803265-3zFRSVUyjLTj6N2VMaIG3PNjA8bcbd"
access_secret <-"J80cCRB5jqQp0tdeOTzjNC0r3k9X9U9zcVVmllJT4fVhg"

twitter_token = rtweet::create_token(app ='DMDA',
                                     consumer_key ="HJFjGj4jePPbVlmHRwqUUA2hR",
                                     consumer_secret ="iMwL77EecWmwJcjZ2IJcACtbKeqrU330pNARSrJDTznLblfyA1",access_token="1332340495473803265-3zFRSVUyjLTj6N2VMaIG3PNjA8bcbd",
                                     access_secret="J80cCRB5jqQp0tdeOTzjNC0r3k9X9U9zcVVmllJT4fVhg"
)

setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)

#Finding location of Trending Tweets available all over the world
View(trends_available())

#Finding location of Trending Tweets available all over parts of India
View(trends_available() %>% filter(countryCode=="IN"))


# TESTING WOEID (Where On Earth IDentifier)
#Finding trending tweets in a specific location (or) we can use parentId to get trending tweets in a particular country

trending_tweets<-get_trends(woeid =23424848)
View(trending_tweets)


#Extracting "ShikharDhawanLeakedVideo" tweets

# extracting 4000 tweets related t0 "ShikharDhawanLeakedVideo" topic
tweets<- searchTwitter("#ShikharDhawanLeakedVideo", n=4000, lang="en")
n.tweet <- length(tweets)

# convert tweets to a data frame
tweets.df <- twListToDF(tweets)

tweets.txt <- sapply(tweets, function(t)t$getText())
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")

## pre-processing text:
clean.text = function(x)
{
                                                       # convert to lower case
                                                       x = tolower(x)
                                                       # remove rt
                                                       x = gsub("rt", "", x)
                                                       # remove at
                                                       x = gsub("@\\w+", "", x)
                                                       # remove punctuation
                                                       x = gsub("[[:punct:]]", "", x)
                                                       # remove numbers
                                                       x = gsub("[[:digit:]]", "", x)
                                                       # remove links http
                                                       x = gsub("http\\w+", "", x)
                                                       # remove tabs
                                                       x = gsub("[ |\t]{2,}", "", x)
                                                       # remove blank spaces at the beginning
                                                       x = gsub("^ ", "", x)
                                                       # remove blank spaces at the end
                                                       x = gsub(" $", "", x)
                                                       # some other cleaning text
                                                       x = gsub('https://','',x)
                                                       x = gsub('http://','',x)
                                                       x = gsub('[^[:graph:]]', ' ',x)
                                                       x = gsub('[[:punct:]]', '', x)
                                                       x = gsub('[[:cntrl:]]', '', x)
                                                       x = gsub('\\d+', '', x)
                                                       x = str_replace_all(x,"[^[:graph:]]", " ")
                                                       return(x)
}

cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]


#Frequency of Tweets

tweets.df %<>% 
                                                       mutate(
                                                                                                              created = created %>% 
                                                                                                                                                                     # Remove zeros.
                                                                                                                                                                     str_remove_all(pattern = '\\+0000') %>%
                                                                                                                                                                     # Parse date.
                                                                                                                                                                     parse_date_time(orders = '%y-%m-%d %H%M%S')
                                                       )

tweets.df %<>% 
                                                       mutate(Created_At_Round = created%>% round(units = 'hours') %>% as.POSIXct())

tweets.df %>% pull(created) %>% min()

tweets.df %>% pull(created) %>% max()

plt <- tweets.df %>% 
                                                       dplyr::count(Created_At_Round) %>% 
                                                       ggplot(mapping = aes(x = Created_At_Round, y = n)) +
                                                       theme_light() +
                                                       geom_line() +
                                                       xlab(label = 'Date') +
                                                       ylab(label = NULL) +
                                                       ggtitle(label = 'Number of Tweets per Hour')

plt %>% ggplotly()


# Loading sentiment word lists

positive = scan("C:\\Users\\DELL\\Downloads\\opinion-lexicon-English\\positive-words.txt", what = 'character', comment.char = ';')
negative = scan("C:\\Users\\DELL\\Downloads\\opinion-lexicon-English\\negative-words.txt", what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'upgrade','Congrats','prizes','prize','thanks','thnx',
              'Grt','gr8','plz','trending','recovering','brainstorm','leader')
neg.words = c(negative,'wtf','wait','waiting','epicfail','Fight','fighting',
              'arrest','no','not')


#Sentiment scoring function

score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
                                                       require(plyr)
                                                       require(stringr)
                                                       
                                                       # we are giving vector of sentences as input. 
                                                       # plyr will handle a list or a vector as an "l" for us
                                                       # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
                                                       scores = laply(sentences, function(sentence, pos.words, neg.words) {
                                                                                                              
                                                                                                              # clean up sentences with R's regex-driven global substitute, gsub() function:
                                                                                                              sentence = gsub('https://','',sentence)
                                                                                                              sentence = gsub('http://','',sentence)
                                                                                                              sentence = gsub('[^[:graph:]]', ' ',sentence)
                                                                                                              sentence = gsub('[[:punct:]]', '', sentence)
                                                                                                              sentence = gsub('[[:cntrl:]]', '', sentence)
                                                                                                              sentence = gsub('\\d+', '', sentence)
                                                                                                              sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
                                                                                                              # and convert to lower case:
                                                                                                              sentence = tolower(sentence)
                                                                                                              
                                                                                                              # split into words. str_split is in the stringr package
                                                                                                              word.list = str_split(sentence, '\\s+')
                                                                                                              # sometimes a list() is one level of hierarchy too much
                                                                                                              words = unlist(word.list)
                                                                                                              
                                                                                                              # compare our words to the dictionaries of positive & negative terms
                                                                                                              pos.matches = match(words, pos.words)
                                                                                                              neg.matches = match(words, neg.words)
                                                                                                              
                                                                                                              # match() returns the position of the matched term or NA
                                                                                                              # we just want a TRUE/FALSE:
                                                                                                              pos.matches = !is.na(pos.matches)
                                                                                                              neg.matches = !is.na(neg.matches)
                                                                                                              
                                                                                                              # TRUE/FALSE will be treated as 1/0 by sum():
                                                                                                              score = sum(pos.matches) - sum(neg.matches)
                                                                                                              
                                                                                                              return(score)
                                                       }, pos.words, neg.words, .progress=.progress )
                                                       
                                                       scores.df = data.frame(score=scores, text=sentences)
                                                       return(scores.df)
}


#Calculating the sentiment score

analysis <- score.sentiment(cleanText, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)


#Histogram of sentiment scores

analysis %>%
                                                       ggplot(aes(x=score)) + 
                                                       geom_histogram(binwidth = 1, fill = "lightblue")+ 
                                                       ylab("Frequency") + 
                                                       xlab("sentiment score") +
                                                       ggtitle("Distribution of Sentiment scores of the tweets") +
                                                       ggeasy::easy_center_title()

#Barplot of sentiment type

neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
                                                       geom_bar(stat = "identity", aes(fill = Sentiment))+
                                                       ggtitle("Barplot of Sentiment type of 4000 tweets")


#WordCloud

text_corpus <- Corpus(VectorSource(cleanText))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords("english")))
text_corpus <- tm_map(text_corpus, removeWords, c("global","globalwarming"))
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
          colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)


#Word Frequency plot

ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) + 
                                                       geom_bar(stat="identity") +
                                                       xlab("Terms") + 
                                                       ylab("Count") + 
                                                       coord_flip() +
                                                       theme(axis.text=element_text(size=7)) +
                                                       ggtitle('Most common word frequency plot') +
                                                       ggeasy::easy_center_title()


